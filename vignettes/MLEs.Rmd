---
title: "MLE in R"
author: "Alexander Forrence"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

(note to self: run `rmarkdown::render('vignettes/MLEs.Rmd')` to make html available)
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=7, fig.height=6, 
                      warning=FALSE, message=FALSE)
library(svglite)
knitr::opts_chunk$set(
  dev = "svglite",
  fig.ext = ".svg"
)
#fix text though, svglite flubs it up
```

First, we'll load some of the necessary packages:

```{r pkgs, message=FALSE,warning=FALSE}
library(lme4)
library(bbmle)
library(optimx)
library(plotly)
library(psyphy)
library(lattice)
library(ggplot2)
library(dplyr)
library(nlme)
library(zoo)
library(boot)
library(tidyr)
theme_set(theme_gray(base_size = 8))

```
For example, to read in the csv containing the data for the free RT...

```{r}
free_rt <- read.csv(system.file("extdata", "bigrt.csv", package = 'multimles'),
                    header = FALSE,
                    col.names = c('id', 'RT', 'reachDir', 'hit'))
```

and for the forced RT,

```{r}
forced_rt <- read.csv(system.file("extdata", "bigtr.csv", package = 'multimles'), 
                      header = FALSE,
                      col.names = c('id', 'RT', 'reachDir', 'hit'))
```

To clean up the data a bit, try things like

```{r, eval = FALSE}
    free_rt <- free_rt[complete.cases(free_rt),]
    free_rt <- free_rt[!(free_rt$RT > 0.5),]
    free_rt <- free_rt[!(free_rt$RT < 0),]
    # Should have 1860 obs. left over (check with str())
```

Or (in `dplyr` syntax):
```{r}
free_rt <- free_rt %>% filter(!is.nan(reachDir), RT < 0.5, RT > 0)
forced_rt <- forced_rt %>% filter(!is.nan(reachDir), RT < 0.5, RT > 0)
```

Alternatives are left as an exercise to the reader.

## Plots

First, a little bit more pushing around...
```{r}
free_rt <- free_rt %>% arrange(RT) %>% 
  group_by(id) %>% mutate(rollrt = rollmean(RT, 10, na.pad = TRUE),
                          rollhit = rollmean(hit, 10, na.pad = TRUE))
forced_rt <- forced_rt %>% arrange(RT) %>% 
  group_by(id) %>% mutate(rollrt = rollmean(RT, 15, na.pad = TRUE),
                          rollhit = rollmean(hit, 15, na.pad = TRUE))

```

And plots:

```{r, warning=FALSE}
ggplot(free_rt, aes(RT)) + 
  geom_histogram(aes(fill = as.factor(hit)), bins = 15) + 
  facet_wrap(~id)
ggplot(forced_rt, aes(rollrt, rollhit)) + 
  geom_line() + 
  facet_wrap(~id) 

```

## Maximum Likelihood Estimation

There were eight targets in this experiment. The objective function in `MATLAB` is specified as:

> `LL = @(params) `
> `-sum(hit.*log((1/8+asymptErr*normcdf(RT,params(1),params(2))*7/8)) +`
> `(1-hit).*log(1-(1/8+asymptErr*normcdf(RT,params(1),params(2))*7/8)));`

Where `asymptErr` was the upper asymptote to the probit curve (starting value of 0.9), `params(1)` was the mean (start 0.3), and `params(2)` was the sd (start 0.1).

A way to do it in R is with the `bbmle` package, a wrapper around the `mle`? function plus a number of very useful helper functions.

```{r, warning=FALSE,message=FALSE}
llsig <- function(mu, sigma, asymptErr, rt, hit, Ntargs){
    p1 <- log(1/Ntargs + asymptErr * pnorm(rt, mu, sigma)*(1 - 1/Ntargs))
    p2 <- log(1 - (1/Ntargs + asymptErr * pnorm(rt, mu, sigma) * (1 - 1/Ntargs)))
    -sum(hit %*% p1 + (1 - hit) %*% p2)
}
    
start_vals <- list(mu = 0.3, sigma = 0.1, asymptErr = 0.9)
subdat <- dplyr::filter(forced_rt, id == 5)
mod1 <- mle2(llsig,
            start = start_vals, 
            method = "BFGS", 
            optimizer = "optim",
            data = list(rt = subdat$RT, hit = subdat$hit),
            fixed = list(Ntargs = 8))
```
    
It takes some work to get answers out of `mle2`, and the optimizers in R tend not to be as robust as those in MATLAB (at least I've heard/experienced).

One option is to go the hierarchical/multilevel/mixed model route, which can save us an awful lot of parameters to estimate, plus make interpretation of individual vs. group performance quite a bit cleaner.

<note about shrinkage>

In nlme, here's a (mostly) parameterized model (random asymptote, mean, and sd, but no correlation in the random effects. The *fully* parameterized model was tending toward overparameterization, as we got an error suggesting the approx. Hessian of the deviance was not positive definite):
```{r}
mnlme <- nlme(hit ~ 1/8 + 7/8 * asym * pnorm(RT, mu, sigma), 
              data = forced_rt,
           fixed = asym + mu + sigma ~ 1, 
           random = pdDiag(asym + mu + sigma ~ 1), 
           start = c(asym = .9, mu = .25, sigma = .04), 
           groups = ~id,
           weights = varExp(form = ~RT)) #consider adding a function to describe within-group variance,
                         #like weights = varExp(form = ~RT). The model fits better and sort of
                         #makes sense, though I haven't wrapped my head around what it *means*.
                         #It does make sense to have unequal variance (large at low RT, small at high)
summary(mnlme)
```

Plus intervals on parameters, and `coef()`:
```{r}
intervals(mnlme)
coef(mnlme) # What we would sort of get if we fit each individual
```

We'll see how those line up with the rolling average (interactively!):
```{r, echo = FALSE}
forced_rt$pred <- predict(mnlme)
forced_rt$pred0 <- predict(mnlme, level = 0)
temp <- ggplot(forced_rt, aes(rollrt, rollhit)) + 
  geom_line(aes(colour = 'Rolling mean'), size = 0.2) + 
  geom_line(aes(y = pred, colour = 'nlme pred'), size = 0.2) +
  facet_wrap(~id)
ggplotly(temp)

ggplot(forced_rt, aes(RT, pred0)) + 
    geom_line(aes(y = pred, colour = 'indiv_pred', group = id), 
              size = 1, alpha = .5) + 
    geom_line(aes(colour = 'Pop pred'), size = 2) 
```

It works fine, conditional on starting values being relatively close to the solution.

## Free Reaction Time

We'll try a random intercept model first, and use parametric bootstrap for inference.

```{r, echo = FALSE}
mlme <- lmer(formula = RT ~ 1 + (1 | id), data = free_rt) # Use REML: does this count as "small"?
boots <- bootMer(mlme, FUN = function(x) 
  predict(x, newdata = data.frame(id = unique(free_rt$id))), 
  seed = 1, nsim = 500)
temp <- data.frame(boots$t)
names(temp) <- dimnames(boots$t)[[2]]
temp <- gather(temp, id, val) # from tidyr
temp2 <- data.frame(xx = boots$t0, id = unique(free_rt$id))

ggplot(free_rt, aes(x = RT)) + 
    geom_histogram(aes(fill = 'raw', y = ..count../sum(..count..)), # rescale counts
                   bins = 20, alpha = .7) + 
    geom_histogram(data = temp, aes(x = val, y = ..count../sum(..count..),
                                    fill = 'pred'), bins = 20, alpha = .7) + 
    geom_vline(data = temp2, aes(xintercept = xx), size = 1, colour = 'black',
               linetype = 'longdash') +
    facet_wrap(~id)
```

This seems to give reasonable estimates. There may be some bias between the original prediction and the median of the parametric bootstrap, but the magnitude of that bias is relatively small.

```{r}
print(boots)
```

## Template Model Builder Example

```{r}
# Also see https://groups.nceas.ucsb.edu/non-linear-modeling/projects
# writeLines(con = file("temp.cpp"), c('
# #include <TMB.hpp>
# 
# template<class Type>
# Type objective_function<Type>::operator() ()
# {
#     DATA_INTEGER(ntarg);    // number of targets
#     DATA_VECTOR(hit);       // Obs ervations
#     DATA_VECTOR(rt);        // Reaction time
#     DATA_FACTOR(id);        // individuals
#     PARAMETER(mu);          // mean
#     PARAMETER(sig);         // sigma
#     PARAMETER(asym);        // asymptote
#     PARAMETER(log_sigma);   // ?
#     PARAMETER(log_sigma_u); // ?
#     PARAMETER_VECTOR(u);    // random effects contributions
# }
# 
# ')); closeAllConnections()
# 
# dat <- list(rt = forced_rt$RT, hit = forced_rt$hit)
# obj <- MakeADFun(data = dat,
#                  parameters = list(
#                    mu = 0.3,
#                    sig = 0.1,
#                    asym = 0.9
#                  ),
#                  random = c('mu','sig','asym'))
```
